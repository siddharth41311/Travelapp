{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56a0a801",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_user' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 35\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, mean_squared_error, r2_score\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# # Load environment variables\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# load_dotenv()\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# DB_URL = os.getenv(\"DB_URL\")\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# df_flight = pd.read_sql(\"SELECT * FROM flight\", engine)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# df_user = pd.read_sql(\"SELECT * FROM passenger\", engine)\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m df_user\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musercode\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     37\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(df_user, df_flight, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     38\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_user' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# # Load environment variables\n",
    "# load_dotenv()\n",
    "# DB_URL = os.getenv(\"DB_URL\")\n",
    "# engine = create_engine( DB_URL)\n",
    "\n",
    "\n",
    "# # Load Data\n",
    "# df_flight = pd.read_sql(\"SELECT * FROM flight\", engine)\n",
    "df_user = pd.read_sql(\"SELECT * FROM passenger\", engine)\n",
    "\n",
    "df_user.rename(columns={'usercode': 'user_id'}, inplace=True)\n",
    "\n",
    "df = pd.merge(df_user, df_flight, on='user_id', how='inner')\n",
    "df.drop(columns=['name'], inplace=True)\n",
    "\n",
    "# Feature Engineering\n",
    "df['flight_date'] = pd.to_datetime(df['departure_date'])\n",
    "df['is_weekend_flight'] = (df['flight_date'].dt.weekday >= 5).astype(int)\n",
    "df['flight_month'] = df['flight_date'].dt.month\n",
    "\n",
    "\n",
    "X = df.drop(columns=['flight_price'])\n",
    "y = df['flight_price']\n",
    "\n",
    "\n",
    "# Preprocessing Pipeline\n",
    "num_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "cat_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "num_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_transformer, num_features),\n",
    "    ('cat', cat_transformer, cat_features)\n",
    "])\n",
    "\n",
    "# Train-Test Split\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "def objective(trial):\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=trial.suggest_int('n_estimators', 100, 500),\n",
    "        learning_rate=trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        max_depth=trial.suggest_int('max_depth', 3, 10),\n",
    "        subsample=trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        colsample_bytree=trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return mean_squared_error(y_test, y_pred)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "best_params = study.best_params\n",
    "\n",
    "# Train Best Model\n",
    "best_xgb = XGBRegressor(**best_params)\n",
    "best_xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = best_xgb.predict(X_test)\n",
    "\n",
    "# Stacking Model\n",
    "stacked_model = StackingRegressor(\n",
    "    estimators=[('xgb', best_xgb)],\n",
    "    final_estimator=XGBRegressor(n_estimators=100, learning_rate=0.05, max_depth=3, random_state=42)\n",
    ")\n",
    "stacked_model.fit(X_train, y_train)\n",
    "y_pred_stack = stacked_model.predict(X_test)\n",
    "\n",
    "# Feature Scaling for Better Training\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Neural Network Model\n",
    "def build_nn():\n",
    "    model = Sequential([\n",
    "        Dense(256, kernel_regularizer=l2(0.001), input_shape=(X_train_scaled.shape[1],)),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(128, kernel_regularizer=l2(0.001)),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(64, kernel_regularizer=l2(0.001)),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(32, kernel_regularizer=l2(0.001)),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "\n",
    "        Dense(1)  # Output layer for regression\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.005), loss=Huber(delta=1.0), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Train the improved model\n",
    "nn_model = build_nn()\n",
    "nn_callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=1e-6),\n",
    "    EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "history = nn_model.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test), \n",
    "                        epochs=150, batch_size=64, verbose=1, callbacks=nn_callbacks)\n",
    "\n",
    "# Make Predictions\n",
    "y_pred_nn = nn_model.predict(X_test_scaled).flatten()\n",
    "\n",
    "# # Neural Network Model\n",
    "# def build_nn():\n",
    "#     model = Sequential([\n",
    "#         Dense(128, activation='relu', kernel_regularizer=l2(0.001), input_shape=(X_train.shape[1],)),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.3),\n",
    "#         Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.3),\n",
    "#         Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "#         Dense(1)\n",
    "#     ])\n",
    "#     model.compile(optimizer=Adam(learning_rate=0.01), loss='mse', metrics=['mae'])\n",
    "#     return model\n",
    "\n",
    "# nn_model = build_nn()\n",
    "# nn_callbacks = [\n",
    "#     ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-5),\n",
    "#     EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "# ]\n",
    "# nn_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, verbose=1, callbacks=nn_callbacks)\n",
    "\n",
    "# y_pred_nn = nn_model.predict(X_test).flatten()\n",
    "\n",
    "# Model Evaluation\n",
    "def evaluate_model(name, y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    print(f\"\\n {name} Performance:\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "evaluate_model(\"Optimized XGBoost\", y_test, y_pred_xgb)\n",
    "evaluate_model(\"Stacking Model\", y_test, y_pred_stack)\n",
    "evaluate_model(\"Neural Network\", y_test, y_pred_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff97713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
